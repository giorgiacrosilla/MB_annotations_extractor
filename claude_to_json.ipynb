{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places extraction \n",
    "JSON to TriG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUQxWKNGHMXS",
    "outputId": "d924c8d2-5f39-4f99-c8e9-296376a0cd74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved rdf_output/4_place_1_ancona.trig\n",
      "Processed and saved rdf_output/5_place_1_12 lungarno acciajuoli florence.trig\n",
      "Processed and saved rdf_output/5_place_2_doneys.trig\n",
      "Processed and saved rdf_output/5_place_3_pistoia.trig\n",
      "Processed and saved rdf_output/6_place_1_pistoia.trig\n",
      "Processed and saved rdf_output/6_place_2_florence.trig\n",
      "Processed and saved rdf_output/7_place_1_florence.trig\n",
      "Processed and saved rdf_output/8_place_1_fiesole hill.trig\n",
      "Processed and saved rdf_output/9_place_1_il palmerino maiano.trig\n",
      "Processed and saved rdf_output/9_place_2_il palmerino maiano.trig\n",
      "Processed and saved rdf_output/10_place_1_il palmerino maiano.trig\n",
      "Processed and saved rdf_output/10_place_2_12 lungarno acciajuoli florence.trig\n",
      "Processed and saved rdf_output/10_place_3_uffizi.trig\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 382\u001b[0m\n\u001b[0;32m    379\u001b[0m     processor\u001b[38;5;241m.\u001b[39mprocess_batch(input_directory)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 379\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    376\u001b[0m input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Process all text files in the directory\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 331\u001b[0m, in \u001b[0;36mTextToRDFProcessor.process_batch\u001b[1;34m(self, input_directory)\u001b[0m\n\u001b[0;32m    328\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Extract places\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m places_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_places\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m places_data \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m places_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaces\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo places extracted from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m, in \u001b[0;36mTextToRDFProcessor.extract_places\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     18\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an AI assistant specialized in extracting places from texts into JSON format.\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m    Your task is to analyze the text and extract ONLY the places in which Mary Berenson says she was in a particular day, create a JSON object with these exact fields:\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m    - name: name of the place. Be careful: if the place name contains also information related to streets or buildings, extract this information too. \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m        ]\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124m    }\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease analyze carefully the diary page and extract all the places in which Mary Berenson said SHE WAS IN A PARTICULAR DAY. \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Extract just the JSON part from Claude's response\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\resources\\messages.py:878\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[0;32m    872\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    875\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    876\u001b[0m     )\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 973\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    974\u001b[0m         request,\n\u001b[0;32m    975\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "import anthropic\n",
    "from datetime import datetime\n",
    "from natsort import natsorted\n",
    "\n",
    "class TextToRDFProcessor:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.output_dir = \"rdf_output\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def extract_places(self, text):\n",
    "        \"\"\"Extract places from text using Claude API\"\"\"\n",
    "        system_prompt = \"\"\"You are an AI assistant specialized in extracting places from texts into JSON format.\n",
    "            Your task is to analyze the text and extract ONLY the places in which Mary Berenson says she was in a particular day, create a JSON object with these exact fields:\n",
    "            - name: name of the place. Be careful: if the place name contains also information related to streets or buildings, extract this information too. \n",
    "            - coordinates: coordinates in format (40.04, 30.55)\n",
    "            - wiki_id: the CORRECT wikidata ID, if there's one representing the entity\n",
    "            - line: text line reference in format /p[N] where N is line number\n",
    "\n",
    "            Return the results in this exact format, with no additional text:\n",
    "            {\n",
    "                \"places\": [\n",
    "                    {\n",
    "                        \"name\": \"place name\",\n",
    "                        \"coordinates\": \"(lat, long)\",\n",
    "                        \"wiki_id\": \"Q12345\",\n",
    "                        \"line\": \"/p[1]\"\n",
    "                    }\n",
    "                ]\n",
    "            }\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-latest\",\n",
    "                max_tokens=2048,\n",
    "                system=system_prompt,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please analyze carefully the diary page and extract all the places in which Mary Berenson said SHE WAS IN A PARTICULAR DAY. \\n\\n{text}\"\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract just the JSON part from Claude's response\n",
    "            response_text = response.content[0].text\n",
    "\n",
    "            # Find the JSON object bounds\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "\n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "\n",
    "            # Parse the JSON\n",
    "            places_data = json.loads(json_str)\n",
    "\n",
    "            # Validate the structure\n",
    "            if \"places\" not in places_data:\n",
    "                places_data = {\"places\": [places_data]}\n",
    "\n",
    "            return places_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting places: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def find_place_positions(self, text, place_name):\n",
    "        \"\"\"\n",
    "        Find the start and end positions of a place name in the text\n",
    "        with multiple fallback strategies\n",
    "        \"\"\"\n",
    "        # Normalize place name and text for matching\n",
    "        place_name_normalized = place_name.lower().strip()\n",
    "        text_normalized = text.lower()\n",
    "\n",
    "        def create_flexible_pattern(name):\n",
    "            # Handle possessive forms by removing 's if present in the search term\n",
    "            if name.endswith(\"'s\") or name.endswith(\"s'\"):\n",
    "                name = name.rstrip(\"s'\")\n",
    "            \n",
    "            # Escape special regex characters\n",
    "            escaped_name = re.escape(name)\n",
    "            \n",
    "            # Pattern to match both possessive and non-possessive forms\n",
    "            return fr'(?:^|(?<=[^\\w])){escaped_name}(?:\\'s?)?(?=$|[^\\w])'\n",
    "\n",
    "        # Try direct flexible matching first\n",
    "        matches = list(re.finditer(create_flexible_pattern(place_name_normalized), text_normalized))\n",
    "        \n",
    "        if matches:\n",
    "            # If multiple matches, prefer the first one\n",
    "            match = matches[0]\n",
    "            absolute_start = match.start()\n",
    "            \n",
    "            return {\n",
    "                'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "            }\n",
    "\n",
    "        # If direct word match fails, try partial match\n",
    "        # Remove possessive form from search term if present\n",
    "        search_term = place_name_normalized\n",
    "        if search_term.endswith(\"'s\") or search_term.endswith(\"s'\"):\n",
    "            search_term = search_term.rstrip(\"s'\")\n",
    "        \n",
    "        matches = list(re.finditer(re.escape(search_term) + r\"(?:\\'s?)?\", text_normalized))\n",
    "        \n",
    "        if matches:\n",
    "            match = matches[0]\n",
    "            absolute_start = match.start()\n",
    "            \n",
    "            return {\n",
    "                'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "            }\n",
    "\n",
    "        # Fallback: search for tokens in the place name\n",
    "        tokens = search_term.split()\n",
    "        for token in tokens:\n",
    "            token_matches = list(re.finditer(r'\\b' + re.escape(token) + r'(?:\\'s?)?\\b', text_normalized))\n",
    "            if token_matches:\n",
    "                match = token_matches[0]\n",
    "                absolute_start = match.start()\n",
    "                \n",
    "                return {\n",
    "                    'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                    'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                    'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "                }\n",
    "        \n",
    "        # If all else fails, return None\n",
    "        return None\n",
    "\n",
    "    def _get_line_number(self, text, character_position):\n",
    "        \"\"\"\n",
    "        Get the line number and the local character position within that line\n",
    "        \n",
    "        Args:\n",
    "            text (str): Full text content\n",
    "            character_position (int): Absolute character position in the text\n",
    "        \n",
    "        Returns:\n",
    "            int: Line number (1-indexed)\n",
    "        \"\"\"\n",
    "        # Split the text into lines\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Track cumulative character count\n",
    "        cumulative_chars = 0\n",
    "        \n",
    "        # Iterate through lines to find the correct line\n",
    "        for line_num, line in enumerate(lines, 1):\n",
    "            # Length of the line plus newline character\n",
    "            line_length = len(line) + 1  # +1 for the newline character\n",
    "            \n",
    "            # Check if the character position falls within this line\n",
    "            if character_position < cumulative_chars + line_length:\n",
    "                return line_num\n",
    "            \n",
    "            # Move to next line's starting position\n",
    "            cumulative_chars += line_length\n",
    "        \n",
    "        # Fallback to last line if position is beyond text length\n",
    "        return len(lines)\n",
    "\n",
    "    def generate_uuids(self):\n",
    "        \"\"\"Generate a set of UUIDs for use in the Trig template\"\"\"\n",
    "        return {\n",
    "            'main': str(uuid.uuid4()),\n",
    "            'uuid2': str(uuid.uuid4()),\n",
    "            'uuid3': str(uuid.uuid4()),\n",
    "            'uuid4': str(uuid.uuid4()),\n",
    "            'uuid5': str(uuid.uuid4()),\n",
    "            'uuid6': str(uuid.uuid4()),\n",
    "            'uuid7': str(uuid.uuid4())\n",
    "        }\n",
    "\n",
    "    def convert_to_trig(self, place_data, input_filename):\n",
    "        \"\"\"Convert a single place data to Trig format\"\"\"\n",
    "        uuids = self.generate_uuids()\n",
    "        creation_date = datetime.now().isoformat()\n",
    "        ndiary = os.path.splitext(os.path.basename(input_filename))[0]\n",
    "\n",
    "        # Merge extracted place data with position information\n",
    "        trig_template = f\"\"\"\n",
    "        @prefix crmdig: <http://www.ics.forth.gr/isl/CRMdig/> .\n",
    "@prefix crminfluence: <http://www.cidoc-crm.org/cidoc-crm/influence/> .\n",
    "@prefix oa: <http://www.w3.org/ns/oa#> .\n",
    "@prefix crmsci: <http://www.ics.forth.gr/isl/CRMsci/> .\n",
    "@prefix Help: <http://help.researchspace.org/resource/> .\n",
    "@prefix bds: <http://www.bigdata.com/rdf/search#> .\n",
    "@prefix crmba: <http://www.cidoc-crm.org/cidoc-crm/CRMba/> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix mbdiaries-annotation: <https://mbdiaries.itatti.harvard.edu/annotation/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix iiif: <http://iiif.io/api/> .\n",
    "@prefix crm: <http://www.cidoc-crm.org/cidoc-crm/> .\n",
    "@prefix sim: <http://purl.org/ontology/similarity/> .\n",
    "@prefix fc: <https://collection.itatti.harvard.edu/resource/custom/fc/> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix User: <http://www.researchspace.org/resource/user/> .\n",
    "@prefix mbdiaries-type: <https://mbdiaries.itatti.harvard.edu/resource/type/> .\n",
    "@prefix forms: <http://www.researchspace.org/resource/system/forms/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rshelp: <http://researchspace.org/help/> .\n",
    "@prefix sp: <http://spinrdf.org/sp#> .\n",
    "@prefix Platform: <http://www.researchspace.org/resource/system/> .\n",
    "@prefix mbdiaries: <https://mbdiaries.itatti.harvard.edu/resource/> .\n",
    "@prefix fr: <https://collection.itatti.harvard.edu/resource/custom/fr/> .\n",
    "@prefix crmgeo: <http://www.ics.forth.gr/isl/CRMgeo/> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix mbdiaries_forms: <http://mbdiaries.itatti.harvard.edu/resource/forms> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix rso: <http://www.researchspace.org/ontology/> .\n",
    "@prefix Admin: <http://www.researchspace.org/resource/admin/> .\n",
    "@prefix vitiiif: <https://iiif.itatti.harvard.edu/iiif/2/> .\n",
    "@prefix ontodia: <http://ontodia.org/schema/v1#> .\n",
    "@prefix frbroo: <http://iflastandards.info/ns/fr/frbr/frbroo/> .\n",
    "@prefix crmarchaeo: <http://www.cidoc-crm.org/cidoc-crm/CRMarchaeo/> .\n",
    "@prefix rsp: <http://www.researchspace.org/resource/> .\n",
    "@prefix Default: <https://collection.itatti.harvard.edu/resource/> .\n",
    "@prefix mbdiaries-document: <https://mbdiaries.itatti.harvard.edu/document/> .\n",
    "@prefix mbdiaries-ontology: <https://mbdiaries.itatti.harvard.edu/ontology/> .\n",
    "@prefix ldp: <http://www.w3.org/ns/ldp#> .\n",
    "        \n",
    "    <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container/context> {{\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid7']}>\n",
    "        a oa:TextPositionSelector;\n",
    "        oa:end \"{place_data.get('start_position', '')}\"^^xsd:nonNegativeInteger;\n",
    "        oa:start \"{place_data.get('start_position', '')}\"^^xsd:nonNegativeInteger .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid3']}>\n",
    "        a oa:TextPositionSelector;\n",
    "        oa:end \"{place_data.get('end_position', '')}\"^^xsd:nonNegativeInteger;\n",
    "        oa:start \"{place_data.get('end_position', '')}\"^^xsd:nonNegativeInteger .\n",
    "\n",
    "      mbdiaries-annotation:{uuids['main']} a oa:Annotation, crmdig:D29_Annotation_Object;\n",
    "        crmdig:L48i_was_annotation_created_by <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}>;\n",
    "        oa:hasBody <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/body>;\n",
    "        oa:hasTarget <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-source-{uuids['uuid4']}> .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container>\n",
    "        a ldp:Resource, prov:Entity;\n",
    "        prov:wasAttributedTo User:agent;\n",
    "        prov:generatedAtTime \"{creation_date}\"^^xsd:dateTime .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-source-{uuids['uuid4']}>\n",
    "        a oa:SpecificResource;\n",
    "        oa:hasSource <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}>;\n",
    "        oa:hasSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-{uuids['uuid4']}>;\n",
    "        rdf:value \"{place_data.get('name', '')}\" .\n",
    "\n",
    "      <https://www.wikidata.org/wiki/{place_data.get('wiki_id', '')}> rdfs:label \"{place_data.get('name', '')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-{uuids['uuid4']}>\n",
    "        a oa:RangeSelector;\n",
    "        oa:hasEndSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid5']}>;\n",
    "        oa:hasStartSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid6']}> .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid6']}>\n",
    "        a oa:XPathSelector;\n",
    "        oa:refinedBy <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid7']}>;\n",
    "        rdf:value \"{place_data.get('line', '/p[1]')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid5']}>\n",
    "        a oa:XPathSelector;\n",
    "        oa:refinedBy <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid3']}>;\n",
    "        rdf:value \"{place_data.get('line', '/p[1]')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}>\n",
    "        a crmdig:D30_Annotation_Event;\n",
    "        crm:P4_has_time_span <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}/modifiedAt>;\n",
    "        crm:P14_carried_out_by User:agent .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/body>\n",
    "        a mbdiaries-ontology:Location;\n",
    "        a crm:E52_place;\n",
    "        crm:P168_place_is_defined_by \"{place_data.get('coordinates', '')}\";\n",
    "        owl:sameAs <https://www.wikidata.org/wiki/{place_data.get('wiki_id', '')}> .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}/modifiedAt>\n",
    "        crm:P81b_begin_of_the_end \"{creation_date}\"^^xsd:dateTime;\n",
    "        crm:P81a_end_of_the_begin \"{creation_date}\"^^xsd:dateTime .\n",
    "\n",
    "      _:node1i8224na8x5257 ldp:contains <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container> .\n",
    "    }}\n",
    "\n",
    "    {{\n",
    "      _:node1i8224na8x5257 a ldp:Container, ldp:Resource, prov:Entity .\n",
    "    }}\"\"\"\n",
    "\n",
    "        return trig_template\n",
    "\n",
    "    def process_batch(self, input_directory):\n",
    "        \"\"\"Process all .txt files in the input directory\"\"\"\n",
    "        # Ensure input directory exists\n",
    "        if not os.path.exists(input_directory):\n",
    "            print(f\"Input directory {input_directory} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        # Get all .txt files and sort them naturally\n",
    "        files = [f for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "        sorted_files = natsorted(files)\n",
    "\n",
    "        # Iterate through sorted files\n",
    "        for filename in sorted_files:\n",
    "            filepath = os.path.join(input_directory, filename)\n",
    "            \n",
    "            try:\n",
    "                # Read the text file\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                # Extract places\n",
    "                places_data = self.extract_places(text)\n",
    "\n",
    "                if not places_data or not places_data.get(\"places\"):\n",
    "                    print(f\"No places extracted from {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Process each place\n",
    "                for i, place in enumerate(places_data[\"places\"]):\n",
    "                    # Find place positions in the text\n",
    "                    position_data = self.find_place_positions(text, place['name'])\n",
    "                    \n",
    "                    if position_data:\n",
    "                        # Merge position data with original place data\n",
    "                        complete_place_data = {**place, **position_data}\n",
    "                    else:\n",
    "                        # Fallback if position finding fails\n",
    "                        complete_place_data = {\n",
    "                            **place,\n",
    "                            'start_position': '',\n",
    "                            'end_position': '',\n",
    "                            'line': f\"/p[{i+1}]\"\n",
    "                        }\n",
    "\n",
    "                    # Convert to Trig\n",
    "                    trig_content = self.convert_to_trig(complete_place_data, filename)\n",
    "\n",
    "                    # Create safe filename\n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    safe_place_name = \"\".join(x for x in place['name'].lower() if x.isalnum() or x in (' ', '-', '_'))\n",
    "                    \n",
    "                    # Save to file with original filename as prefix\n",
    "                    output_filename = f\"{self.output_dir}/{base_name}_place_{i+1}_{safe_place_name}.trig\"\n",
    "                    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(trig_content)\n",
    "                    print(f\"Processed and saved {output_filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize processor with your API key\n",
    "    api_key = ''  # Replace with your actual API key\n",
    "    processor = TextToRDFProcessor(api_key)\n",
    "\n",
    "    # Specify the input directory containing text files\n",
    "    input_directory = \"./txt\"  \n",
    "\n",
    "    # Process all text files in the directory\n",
    "    processor.process_batch(input_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates extraction \n",
    "\n",
    "JSON to TriG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hnbD_SIJjHk",
    "outputId": "e2761b1e-c29f-4f9a-93ec-5382b7b4d042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved rdf_output/4_date_1_may 1893.trig\n",
      "Processed and saved rdf_output/5_date_1_february 14 1894.trig\n",
      "Processed and saved rdf_output/6_date_1_saturday the 3rd.trig\n",
      "Processed and saved rdf_output/7_date_1_feb 15 1894.trig\n",
      "No dates extracted from 8.txt\n",
      "Processed and saved rdf_output/9_date_1_friday feb 16 1894.trig\n",
      "Processed and saved rdf_output/9_date_2_saturday feb 17 1894.trig\n",
      "Processed and saved rdf_output/10_date_1_sunday feb 18 1894.trig\n",
      "Processed and saved rdf_output/10_date_2_monday feb 19 1894.trig\n",
      "Processed and saved rdf_output/11_date_1_tuesday feb 20 1894.trig\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 412\u001b[0m\n\u001b[0;32m    409\u001b[0m     processor\u001b[38;5;241m.\u001b[39mprocess_batch(input_directory)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 412\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 409\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    406\u001b[0m input_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Process all text files in the directory\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 363\u001b[0m, in \u001b[0;36mTextToRDFProcessor.process_batch\u001b[1;34m(self, input_directory)\u001b[0m\n\u001b[0;32m    360\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Extract dates\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m dates_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dates_data \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dates_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo dates extracted from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m, in \u001b[0;36mTextToRDFProcessor.extract_dates\u001b[1;34m(self, text, previous_text)\u001b[0m\n\u001b[0;32m     22\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an AI assistant specialized in extracting dates from texts into JSON format.\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124mYour task is to analyze the text and extract ONLY dates which Mary Berenson mentioned and that could be associated to a location in which she says she was, create a JSON object with these exact fields:\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m- date: date value in ISO 8601 format\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124m    ]\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease analyze carefully the diary page and extract all the dates which Mary Berenson mentioned and that could be associated to a location in which she says she was.  \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcontext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Extract and process JSON as before...\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\resources\\messages.py:878\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[0;32m    872\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    875\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    876\u001b[0m     )\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\anthropic\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 973\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    974\u001b[0m         request,\n\u001b[0;32m    975\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import anthropic\n",
    "from datetime import datetime\n",
    "from natsort import natsorted\n",
    "\n",
    "class TextToRDFProcessor:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.output_dir = \"rdf_output\"\n",
    "        self.last_known_date = None  # Store the last known date\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def extract_dates(self, text, previous_text=None):\n",
    "        \"\"\"Extract dates from text using Claude API\"\"\"\n",
    "        context = \"\"\n",
    "        if previous_text:\n",
    "            context = f\"Previous text for context:\\n{previous_text}\\n\\nCurrent text to analyze:\\n\"\n",
    "            \n",
    "        system_prompt = \"\"\"You are an AI assistant specialized in extracting dates from texts into JSON format.\n",
    "        Your task is to analyze the text and extract ONLY dates which Mary Berenson mentioned and that could be associated to a location in which she says she was, create a JSON object with these exact fields:\n",
    "        - date: date value in ISO 8601 format\n",
    "        - name: name of the date\n",
    "        - line: text line reference in format /p[N] where N is line number\n",
    "\n",
    "        \n",
    "        Return the results in this exact format, with no additional text:\n",
    "        {\n",
    "            \"dates\": [\n",
    "                {\n",
    "                    \"name\": \"25th September 2023\",\n",
    "                    \"date\": \"2023-09-25T00:00:00:000Z\",\n",
    "                    \"line\": \"/p[1]\"\n",
    "                }\n",
    "            ]\n",
    "        }\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-latest\",\n",
    "                max_tokens=2048,\n",
    "                system=system_prompt,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please analyze carefully the diary page and extract all the dates which Mary Berenson mentioned and that could be associated to a location in which she says she was.  {context}{text}\"\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract and process JSON as before...\n",
    "            response_text = response.content[0].text\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "\n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            dates_data = json.loads(json_str)\n",
    "\n",
    "            if \"dates\" not in dates_data:\n",
    "                dates_data = {\"dates\": [dates_data]}\n",
    "\n",
    "            # Update last known date if we found any dates\n",
    "            if dates_data[\"dates\"]:\n",
    "                self.last_known_date = dates_data[\"dates\"][-1][\"date\"]\n",
    "\n",
    "            return dates_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting dates: {str(e)}\")\n",
    "            print(f\"Raw response: {response_text}\")\n",
    "            return None\n",
    "    def find_place_positions(self, text, place_name):\n",
    "        \"\"\"\n",
    "        Find the start and end positions of a place name in the text\n",
    "        with multiple fallback strategies\n",
    "        \"\"\"\n",
    "        # Normalize place name and text for matching\n",
    "        place_name_normalized = place_name.lower().strip()\n",
    "        text_normalized = text.lower()\n",
    "\n",
    "        # Try direct word matching first\n",
    "        matches = list(re.finditer(r'\\b' + re.escape(place_name_normalized) + r'\\b', text_normalized))\n",
    "        \n",
    "        if matches:\n",
    "            # If multiple matches, prefer the first one\n",
    "            match = matches[0]\n",
    "            absolute_start = match.start()\n",
    "            \n",
    "            return {\n",
    "                'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "            }\n",
    "        \n",
    "        # If direct word match fails, try partial match\n",
    "        matches = list(re.finditer(re.escape(place_name_normalized), text_normalized))\n",
    "        \n",
    "        if matches:\n",
    "            # If multiple matches, prefer the first one\n",
    "            match = matches[0]\n",
    "            absolute_start = match.start()\n",
    "            \n",
    "            return {\n",
    "                'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "            }\n",
    "        \n",
    "        # Fallback: search for tokens in the place name\n",
    "        tokens = place_name_normalized.split()\n",
    "        for token in tokens:\n",
    "            token_matches = list(re.finditer(r'\\b' + re.escape(token) + r'\\b', text_normalized))\n",
    "            if token_matches:\n",
    "                match = token_matches[0]\n",
    "                absolute_start = match.start()\n",
    "                \n",
    "                return {\n",
    "                    'start_position': absolute_start - sum(len(line) + 1 for line in text_normalized[:absolute_start].split('\\n')[:-1]),\n",
    "                    'end_position': match.end() - sum(len(line) + 1 for line in text_normalized[:match.end()].split('\\n')[:-1]),\n",
    "                    'line': f\"/p[{self._get_line_number(text, absolute_start)}]\"\n",
    "                }\n",
    "        \n",
    "        # If all else fails, return None\n",
    "        return None\n",
    "\n",
    "    def _get_line_number(self, text, character_position):\n",
    "        \"\"\"\n",
    "        Get the line number and the local character position within that line\n",
    "        \n",
    "        Args:\n",
    "            text (str): Full text content\n",
    "            character_position (int): Absolute character position in the text\n",
    "        \n",
    "        Returns:\n",
    "            int: Line number (1-indexed)\n",
    "        \"\"\"\n",
    "        # Split the text into lines\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Track cumulative character count\n",
    "        cumulative_chars = 0\n",
    "        \n",
    "        # Iterate through lines to find the correct line\n",
    "        for line_num, line in enumerate(lines, 1):\n",
    "            # Length of the line plus newline character\n",
    "            line_length = len(line) + 1  # +1 for the newline character\n",
    "            \n",
    "            # Check if the character position falls within this line\n",
    "            if character_position < cumulative_chars + line_length:\n",
    "                return line_num\n",
    "            \n",
    "            # Move to next line's starting position\n",
    "            cumulative_chars += line_length\n",
    "        \n",
    "        # Fallback to last line if position is beyond text length\n",
    "        return len(lines)\n",
    "\n",
    "    def process_batch(self, input_directory):\n",
    "        \"\"\"Process all .txt files in the input directory\"\"\"\n",
    "        if not os.path.exists(input_directory):\n",
    "            print(f\"Input directory {input_directory} does not exist.\")\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        files = [f for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "        sorted_files = natsorted(files)\n",
    "\n",
    "        previous_text = None\n",
    "        \n",
    "        for filename in sorted_files:\n",
    "            filepath = os.path.join(input_directory, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    current_text = f.read()\n",
    "\n",
    "                # Extract dates with context from previous text\n",
    "                dates_data = self.extract_dates(current_text, previous_text)\n",
    "\n",
    "                if not dates_data or not dates_data.get(\"dates\"):\n",
    "                    print(f\"No dates extracted from {filename}\")\n",
    "                    previous_text = current_text\n",
    "                    continue\n",
    "\n",
    "                # Process each date\n",
    "                for i, date in enumerate(dates_data[\"dates\"]):\n",
    "                    trig_content = self.convert_to_trig(date, filename)\n",
    "                    \n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    safe_date_name = \"\".join(x for x in date['name'].lower() if x.isalnum() or x in (' ', '-', '_'))\n",
    "                    \n",
    "                    output_filename = f\"{self.output_dir}/{base_name}_date_{i+1}_{safe_date_name}.trig\"\n",
    "                    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(trig_content)\n",
    "                    print(f\"Processed and saved {output_filename}\")\n",
    "\n",
    "                # Update previous text for next iteration\n",
    "                previous_text = current_text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "    def generate_uuids(self):\n",
    "        \"\"\"Generate a set of UUIDs for use in the Trig template\"\"\"\n",
    "        return {\n",
    "            'main': str(uuid.uuid4()),\n",
    "            'uuid2': str(uuid.uuid4()),\n",
    "            'uuid3': str(uuid.uuid4()),\n",
    "            'uuid4': str(uuid.uuid4()),\n",
    "            'uuid5': str(uuid.uuid4()),\n",
    "            'uuid6': str(uuid.uuid4()),\n",
    "            'uuid7': str(uuid.uuid4())\n",
    "        }\n",
    "\n",
    "    def convert_to_trig(self, date_data, input_filename):\n",
    "        \"\"\"Convert a single date data to Trig format\"\"\"\n",
    "        uuids = self.generate_uuids()\n",
    "        creation_date = datetime.now().isoformat()\n",
    "        ndiary = os.path.splitext(os.path.basename(input_filename))[0]\n",
    "\n",
    "        trig_template = f\"\"\"\n",
    "        \n",
    "    @prefix crmdig: <http://www.ics.forth.gr/isl/CRMdig/> .\n",
    "@prefix crminfluence: <http://www.cidoc-crm.org/cidoc-crm/influence/> .\n",
    "@prefix oa: <http://www.w3.org/ns/oa#> .\n",
    "@prefix crmsci: <http://www.ics.forth.gr/isl/CRMsci/> .\n",
    "@prefix Help: <http://help.researchspace.org/resource/> .\n",
    "@prefix bds: <http://www.bigdata.com/rdf/search#> .\n",
    "@prefix crmba: <http://www.cidoc-crm.org/cidoc-crm/CRMba/> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix mbdiaries-annotation: <https://mbdiaries.itatti.harvard.edu/annotation/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix iiif: <http://iiif.io/api/> .\n",
    "@prefix crm: <http://www.cidoc-crm.org/cidoc-crm/> .\n",
    "@prefix sim: <http://purl.org/ontology/similarity/> .\n",
    "@prefix fc: <https://collection.itatti.harvard.edu/resource/custom/fc/> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix User: <http://www.researchspace.org/resource/user/> .\n",
    "@prefix mbdiaries-type: <https://mbdiaries.itatti.harvard.edu/resource/type/> .\n",
    "@prefix forms: <http://www.researchspace.org/resource/system/forms/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rshelp: <http://researchspace.org/help/> .\n",
    "@prefix sp: <http://spinrdf.org/sp#> .\n",
    "@prefix Platform: <http://www.researchspace.org/resource/system/> .\n",
    "@prefix mbdiaries: <https://mbdiaries.itatti.harvard.edu/resource/> .\n",
    "@prefix fr: <https://collection.itatti.harvard.edu/resource/custom/fr/> .\n",
    "@prefix crmgeo: <http://www.ics.forth.gr/isl/CRMgeo/> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix mbdiaries_forms: <http://mbdiaries.itatti.harvard.edu/resource/forms> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix rso: <http://www.researchspace.org/ontology/> .\n",
    "@prefix Admin: <http://www.researchspace.org/resource/admin/> .\n",
    "@prefix vitiiif: <https://iiif.itatti.harvard.edu/iiif/2/> .\n",
    "@prefix ontodia: <http://ontodia.org/schema/v1#> .\n",
    "@prefix frbroo: <http://iflastandards.info/ns/fr/frbr/frbroo/> .\n",
    "@prefix crmarchaeo: <http://www.cidoc-crm.org/cidoc-crm/CRMarchaeo/> .\n",
    "@prefix rsp: <http://www.researchspace.org/resource/> .\n",
    "@prefix Default: <https://collection.itatti.harvard.edu/resource/> .\n",
    "@prefix mbdiaries-document: <https://mbdiaries.itatti.harvard.edu/document/> .\n",
    "@prefix mbdiaries-ontology: <https://mbdiaries.itatti.harvard.edu/ontology/> .\n",
    "@prefix ldp: <http://www.w3.org/ns/ldp#> .\n",
    "\n",
    "    <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container/context> {{\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid7']}>\n",
    "        a oa:TextPositionSelector;\n",
    "        oa:end \"{date_data.get('start_position', '')}\"^^xsd:nonNegativeInteger;\n",
    "        oa:start \"{date_data.get('start_position', '')}\"^^xsd:nonNegativeInteger .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid3']}>\n",
    "        a oa:TextPositionSelector;\n",
    "        oa:end \"{date_data.get('end_position', '')}\"^^xsd:nonNegativeInteger;\n",
    "        oa:start \"{date_data.get('end_position', '')}\"^^xsd:nonNegativeInteger .\n",
    "\n",
    "      mbdiaries-annotation:{uuids['main']} a oa:Annotation, crmdig:D29_Annotation_Object;\n",
    "        crmdig:L48i_was_annotation_created_by <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}>;\n",
    "        oa:hasBody <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/body>;\n",
    "        oa:hasTarget <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-source-{uuids['uuid4']}> .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container>\n",
    "        a ldp:Resource, prov:Entity;\n",
    "        prov:wasAttributedTo User:agent;\n",
    "        prov:generatedAtTime \"{creation_date}\"^^xsd:dateTime .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-source-{uuids['uuid4']}>\n",
    "        a oa:SpecificResource;\n",
    "        oa:hasSource <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}>;\n",
    "        oa:hasSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-{uuids['uuid4']}>;\n",
    "        rdf:value \"{date_data.get('name', '')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/range-{uuids['uuid4']}>\n",
    "        a oa:RangeSelector;\n",
    "        oa:hasEndSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid5']}>;\n",
    "        oa:hasStartSelector <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid6']}> .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid6']}>\n",
    "        a oa:XPathSelector;\n",
    "        oa:refinedBy <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid7']}>;\n",
    "        rdf:value \"{date_data.get('line', '/p[1]')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/xpath-{uuids['uuid5']}>\n",
    "        a oa:XPathSelector;\n",
    "        oa:refinedBy <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/offset-{uuids['uuid3']}>;\n",
    "        rdf:value \"{date_data.get('line', '/p[1]')}\" .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}>\n",
    "        a crmdig:D30_Annotation_Event;\n",
    "        crm:P4_has_time_span <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}/modifiedAt>;\n",
    "        crm:P14_carried_out_by User:agent .\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/body>\n",
    "        a crm:E52_time_span;\n",
    "        crm:P181b_begin_of_the_end \"{date_data.get('date', \"\")}\"^^xsd:dateTime.\n",
    "\n",
    "      <https://mbdiaries.itatti.harvard.edu/diary/1894-95/document/{ndiary}/annotation-event-{uuids['uuid2']}/modifiedAt>\n",
    "        crm:P81b_begin_of_the_end \"{creation_date}\"^^xsd:dateTime;\n",
    "        crm:P81a_end_of_the_begin \"{creation_date}\"^^xsd:dateTime .\n",
    "\n",
    "      _:node1i8224na8x5257 ldp:contains <https://mbdiaries.itatti.harvard.edu/annotation/{uuids['main']}/container> .\n",
    "    }}\n",
    "\n",
    "    {{\n",
    "      _:node1i8224na8x5257 a ldp:Container, ldp:Resource, prov:Entity .\n",
    "    }}\"\"\"\n",
    "\n",
    "        return trig_template\n",
    "\n",
    "\n",
    "\n",
    "    def process_batch(self, input_directory):\n",
    "        \"\"\"Process all .txt files in the input directory\"\"\"\n",
    "        # Ensure input directory exists\n",
    "        if not os.path.exists(input_directory):\n",
    "            print(f\"Input directory {input_directory} does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        # Get all .txt files and sort them naturally\n",
    "        files = [f for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "        sorted_files = natsorted(files)\n",
    "\n",
    "        # Iterate through sorted files\n",
    "        for filename in sorted_files:\n",
    "            filepath = os.path.join(input_directory, filename)\n",
    "            \n",
    "            try:\n",
    "                # Read the text file\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                # Extract dates\n",
    "                dates_data = self.extract_dates(text)\n",
    "\n",
    "                if not dates_data or not dates_data.get(\"dates\"):\n",
    "                    print(f\"No dates extracted from {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Process each place\n",
    "                for i, date in enumerate(dates_data[\"dates\"]):\n",
    "                    # Find place positions in the text\n",
    "                    position_data = self.find_place_positions(text, date['name'])\n",
    "                    \n",
    "                    if position_data:\n",
    "                        # Merge position data with original place data\n",
    "                        complete_place_data = {**date, **position_data}\n",
    "                    else:\n",
    "                        # Fallback if position finding fails\n",
    "                        complete_place_data = {\n",
    "                            **date,\n",
    "                            'start_position': '',\n",
    "                            'end_position': '',\n",
    "                            'line': f\"/p[{i+1}]\"\n",
    "                        }\n",
    "                    trig_content = self.convert_to_trig(complete_place_data, filename)\n",
    "\n",
    "                    # Create safe filename\n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    safe_date_name = \"\".join(x for x in date['name'].lower() if x.isalnum() or x in (' ', '-', '_'))\n",
    "                    \n",
    "                    # Save to file with original filename as prefix\n",
    "                    output_filename = f\"{self.output_dir}/{base_name}_date_{i+1}_{safe_date_name}.trig\"\n",
    "                    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                        f.write(trig_content)\n",
    "                    print(f\"Processed and saved {output_filename}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize processor with your API key\n",
    "    api_key = ''  # Replace with your actual API key\n",
    "    processor = TextToRDFProcessor(api_key)\n",
    "\n",
    "    # Specify the input directory containing text files\n",
    "    input_directory = \"txt\"  \n",
    "\n",
    "    # Process all text files in the directory\n",
    "    processor.process_batch(input_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event creation \n",
    "This code analyzes text and its corresponding place and time TRiG files to create events following a predefined template aligned with the main data model (detailed in the README). It supports generating sequential events across multiple diary pages, provided the pages are supplied in chronological order. The code is designed to not only process the current text but also anticipate subsequent events by examining following texts, extracting and structuring their relevant values in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gi67ACuUKbE8",
    "outputId": "d25fa50c-c4a7-4397-8ca6-cb4ffcf86836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Trig file: rdf_output\\4_event_e7b2a0c6-f9d4-4f9f-9f7d-2d9f3a6d1d5e.trig\n",
      "\n",
      "Processed 4:\n",
      "\n",
      "Event: Ancona, May 1893\n",
      "Place UUID: 76277aeb-4eea-46fc-91e7-679d201011ab\n",
      "Date UUID: 3ab82e50-efdd-4144-b64c-68a98f277b33\n",
      "Event UUID: e7b2a0c6-f9d4-4f9f-9f7d-2d9f3a6d1d5e\n",
      "Next Event: 12 Lungarno Acciajuoli, Florence, 14 February 1894\n",
      "Next Event UUID: e1234567-e89b-12d3-a456-426614174000\n",
      "Saved Trig file: rdf_output\\5_event_e1234567-e89b-12d3-a456-426614174000.trig\n",
      "Saved Trig file: rdf_output\\5_event_e2345678-e89b-12d3-a456-426614174000.trig\n",
      "Saved Trig file: rdf_output\\5_event_e3456789-e89b-12d3-a456-426614174000.trig\n",
      "\n",
      "Processed 5:\n",
      "\n",
      "Event: 12 Lungarno Acciajuoli, Florence, 14 February 1894\n",
      "Place UUID: 5453127b-4a16-48bf-b4c0-d1585cdc1175\n",
      "Date UUID: d9d8accc-a416-4707-bf92-824b042e55fb\n",
      "Event UUID: e1234567-e89b-12d3-a456-426614174000\n",
      "Next Event: Doney's, 14 February 1894\n",
      "Next Event UUID: e2345678-e89b-12d3-a456-426614174000\n",
      "\n",
      "Event: Doney's, 14 February 1894\n",
      "Place UUID: cca09c84-f177-41ea-bce7-1e40d6596d10\n",
      "Date UUID: d9d8accc-a416-4707-bf92-824b042e55fb\n",
      "Event UUID: e2345678-e89b-12d3-a456-426614174000\n",
      "Next Event: Pistoia, 03 February 1894\n",
      "Next Event UUID: e3456789-e89b-12d3-a456-426614174000\n",
      "\n",
      "Event: Pistoia, 03 February 1894\n",
      "Place UUID: 3474aca4-ddae-4ecc-a735-5a80dbf2b865\n",
      "Date UUID: None\n",
      "Event UUID: e3456789-e89b-12d3-a456-426614174000\n",
      "Next Event: Pistoia, Saturday the 3rd\n",
      "Next Event UUID: d8d7f9a0-f9f5-4d4f-9f9d-e6f6d5f8d7c2\n",
      "Saved Trig file: rdf_output\\6_event_d8d7f9a0-f9f5-4d4f-9f9d-e6f6d5f8d7c2.trig\n",
      "\n",
      "Processed 6:\n",
      "\n",
      "Event: Pistoia, Saturday the 3rd\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/013f1afa-5f0b-4740-94f8-48f2709aba81/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/f37ae9f0-c820-410d-9461-265a0d87cbef/body\n",
      "Event UUID: d8d7f9a0-f9f5-4d4f-9f9d-e6f6d5f8d7c2\n",
      "Next Event: Florence, 15 February 1894\n",
      "Next Event UUID: 3d8b7d9a-f8f4-4c9f-9f7d-e8a6d5e8f7c3\n",
      "Saved Trig file: rdf_output\\7_event_3d8b7d9a-f8f4-4c9f-9f7d-e8a6d5e8f7c3.trig\n",
      "\n",
      "Processed 7:\n",
      "\n",
      "Event: Florence, 15 February 1894\n",
      "Place UUID: 2b6448e6-dae6-4cb8-b463-57067c892d1f\n",
      "Date UUID: e8922e6f-ba4b-4afc-bd4a-d3d7b3ac4216\n",
      "Event UUID: 3d8b7d9a-f8f4-4c9f-9f7d-e8a6d5e8f7c3\n",
      "Next Event: Il Palmerino, Maiano, Friday Feb 16 1894\n",
      "Next Event UUID: 3e7a7f1a-5d8d-4d0b-9c9a-2d3f8a9d7b2e\n",
      "Skipping 8: No matching trig files found\n",
      "Saved Trig file: rdf_output\\9_event_3e7a7f1a-5d8d-4d0b-9c9a-2d3f8a9d7b2e.trig\n",
      "Saved Trig file: rdf_output\\9_event_d8d9c7d6-f9c6-4f6d-9f7f-b1a6d3f4d2f4.trig\n",
      "\n",
      "Processed 9:\n",
      "\n",
      "Event: Il Palmerino, Maiano, Friday Feb 16 1894\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/dde61f82-b416-4e72-8f62-0ecf2b585e33/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/4c39afd3-5da0-40a9-aaa4-a9e2e9cbf038/body\n",
      "Event UUID: 3e7a7f1a-5d8d-4d0b-9c9a-2d3f8a9d7b2e\n",
      "Next Event: Il Palmerino, Maiano, Saturday Feb 17 1894\n",
      "Next Event UUID: d8d9c7d6-f9c6-4f6d-9f7f-b1a6d3f4d2f4\n",
      "\n",
      "Event: Il Palmerino, Maiano, Saturday Feb 17 1894\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/a012f74e-ba84-4152-a8df-df81ab6d4e48/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/e8942c63-a99e-4fcd-a89c-e46dcedcff28/body\n",
      "Event UUID: d8d9c7d6-f9c6-4f6d-9f7f-b1a6d3f4d2f4\n",
      "Next Event: Il Palmerino, Maiano, Sunday Feb 18 1894\n",
      "Next Event UUID: 3e7a7f1a-5a9a-4d8b-9b7e-2d1d7f64c1e7\n",
      "Saved Trig file: rdf_output\\10_event_3e7a7f1a-5a9a-4d8b-9b7e-2d1d7f64c1e7.trig\n",
      "Saved Trig file: rdf_output\\10_event_d8d8f7d6-9a9e-4f0e-9f7d-a6e6d5d6d0d4.trig\n",
      "Saved Trig file: rdf_output\\10_event_f8d685f2-283c-4677-a7f6-d5d9d9d9d9d9.trig\n",
      "\n",
      "Processed 10:\n",
      "\n",
      "Event: Il Palmerino, Maiano, Sunday Feb 18 1894\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/11c43278-883f-4c8b-a4e7-94306c44c3cb/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/bfb4cdd8-7a6a-42fa-8936-a0a6fe86c1be/body\n",
      "Event UUID: 3e7a7f1a-5a9a-4d8b-9b7e-2d1d7f64c1e7\n",
      "Next Event: 12 Lungarno Acciajuoli, Florence, Monday Feb 19 1894\n",
      "Next Event UUID: d8d8f7d6-9a9e-4f0e-9f7d-a6e6d5d6d0d4\n",
      "\n",
      "Event: 12 Lungarno Acciajuoli, Florence, Monday Feb 19 1894\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/f2d2ff50-e9b2-4418-bc90-e14950fd697e/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/1369f2c7-f66c-4c42-883e-015ca2365b12/body\n",
      "Event UUID: d8d8f7d6-9a9e-4f0e-9f7d-a6e6d5d6d0d4\n",
      "Next Event: Uffizi, Monday Feb 19 1894\n",
      "Next Event UUID: f8d685f2-283c-4677-a7f6-d5d9d9d9d9d9\n",
      "\n",
      "Event: Uffizi, Monday Feb 19 1894\n",
      "Place UUID: https://mbdiaries.itatti.harvard.edu/annotation/712061f8-d7a7-454a-b698-9f292dcdc28f/body\n",
      "Date UUID: https://mbdiaries.itatti.harvard.edu/annotation/1369f2c7-f66c-4c42-883e-015ca2365b12/body\n",
      "Event UUID: f8d685f2-283c-4677-a7f6-d5d9d9d9d9d9\n",
      "Next Event: Il Palmerino, Maiano, Sunday Feb 18 1894\n",
      "Next Event UUID: 3e7a7f1a-5a9a-4d8b-9b7e-2d1d7f64c1e7\n",
      "Skipping 11: No matching trig files found\n",
      "Skipping 12: No matching trig files found\n",
      "Skipping 13: No matching trig files found\n",
      "Skipping 14: No matching trig files found\n",
      "Skipping 15: No matching trig files found\n",
      "Skipping 16: No matching trig files found\n",
      "Skipping 17: No matching trig files found\n",
      "Skipping 18: No matching trig files found\n",
      "Skipping 19: No matching trig files found\n",
      "Skipping 20: No matching trig files found\n",
      "Skipping 21: No matching trig files found\n",
      "Skipping 22: No matching trig files found\n",
      "Skipping 23: No matching trig files found\n",
      "Skipping 24: No matching trig files found\n",
      "Skipping 25: No matching trig files found\n",
      "Skipping 26: No matching trig files found\n",
      "Skipping 27: No matching trig files found\n",
      "Skipping 28: No matching trig files found\n",
      "Skipping 29: No matching trig files found\n",
      "Skipping 30: No matching trig files found\n",
      "Skipping 31: No matching trig files found\n",
      "Skipping 32: No matching trig files found\n",
      "Skipping 33: No matching trig files found\n",
      "Skipping 35: No matching trig files found\n",
      "Skipping 36: No matching trig files found\n",
      "Skipping 37: No matching trig files found\n",
      "Skipping 39: No matching trig files found\n",
      "Skipping 40: No matching trig files found\n",
      "Skipping 41: No matching trig files found\n",
      "Skipping 43: No matching trig files found\n",
      "Skipping 44: No matching trig files found\n",
      "Skipping 45: No matching trig files found\n",
      "Skipping 46: No matching trig files found\n",
      "Skipping 47: No matching trig files found\n",
      "Skipping 48: No matching trig files found\n",
      "Skipping 49: No matching trig files found\n",
      "Skipping 50: No matching trig files found\n",
      "Skipping 51: No matching trig files found\n",
      "Skipping 52: No matching trig files found\n",
      "Skipping 53: No matching trig files found\n",
      "Skipping 54: No matching trig files found\n",
      "Skipping 55: No matching trig files found\n",
      "Skipping 56: No matching trig files found\n",
      "Skipping 57: No matching trig files found\n",
      "Skipping 58: No matching trig files found\n",
      "Skipping 59: No matching trig files found\n",
      "Skipping 61: No matching trig files found\n",
      "Skipping 62: No matching trig files found\n",
      "Skipping 63: No matching trig files found\n",
      "Skipping 64: No matching trig files found\n",
      "Skipping 65: No matching trig files found\n",
      "Skipping 66: No matching trig files found\n",
      "Skipping 67: No matching trig files found\n",
      "Skipping 68: No matching trig files found\n",
      "Skipping 69: No matching trig files found\n",
      "Skipping 71: No matching trig files found\n",
      "Skipping 74: No matching trig files found\n",
      "Skipping 75: No matching trig files found\n",
      "Skipping 76: No matching trig files found\n",
      "Skipping 77: No matching trig files found\n",
      "Skipping 78: No matching trig files found\n",
      "Skipping 79: No matching trig files found\n",
      "Skipping 82: No matching trig files found\n",
      "Skipping 83: No matching trig files found\n",
      "Skipping 84: No matching trig files found\n",
      "Skipping 85: No matching trig files found\n",
      "Skipping 86: No matching trig files found\n",
      "Skipping 87: No matching trig files found\n",
      "Skipping 89: No matching trig files found\n",
      "Skipping 90: No matching trig files found\n",
      "Skipping 91: No matching trig files found\n",
      "Skipping 92: No matching trig files found\n",
      "Skipping 93: No matching trig files found\n",
      "Skipping 94: No matching trig files found\n",
      "Skipping 95: No matching trig files found\n",
      "Skipping 96: No matching trig files found\n",
      "Skipping 97: No matching trig files found\n",
      "Skipping 98: No matching trig files found\n",
      "Skipping 99: No matching trig files found\n",
      "Skipping 100: No matching trig files found\n",
      "Skipping 101: No matching trig files found\n",
      "Skipping 102: No matching trig files found\n",
      "Skipping 103: No matching trig files found\n",
      "Skipping 104: No matching trig files found\n",
      "Skipping 105: No matching trig files found\n",
      "Skipping 106: No matching trig files found\n",
      "Skipping 107: No matching trig files found\n",
      "Skipping 108: No matching trig files found\n",
      "Skipping 109: No matching trig files found\n",
      "Skipping 110: No matching trig files found\n",
      "Skipping 111: No matching trig files found\n",
      "Skipping 112: No matching trig files found\n",
      "Skipping 113: No matching trig files found\n",
      "Skipping 114: No matching trig files found\n",
      "Skipping 115: No matching trig files found\n",
      "Skipping 116: No matching trig files found\n",
      "Skipping 117: No matching trig files found\n",
      "Skipping 118: No matching trig files found\n",
      "Skipping 119: No matching trig files found\n",
      "Skipping 120: No matching trig files found\n",
      "Skipping 121: No matching trig files found\n",
      "Skipping 122: No matching trig files found\n",
      "Skipping 123: No matching trig files found\n",
      "Skipping 124: No matching trig files found\n",
      "Skipping 125: No matching trig files found\n",
      "Skipping 127: No matching trig files found\n",
      "Skipping 129: No matching trig files found\n",
      "Skipping 132: No matching trig files found\n",
      "Skipping 133: No matching trig files found\n",
      "Skipping 134: No matching trig files found\n",
      "Skipping 135: No matching trig files found\n",
      "Skipping 136: No matching trig files found\n",
      "Skipping 137: No matching trig files found\n",
      "Skipping 138: No matching trig files found\n",
      "Skipping 139: No matching trig files found\n",
      "Skipping 140: No matching trig files found\n",
      "Skipping 142: No matching trig files found\n",
      "Skipping 143: No matching trig files found\n",
      "Skipping 144: No matching trig files found\n",
      "Skipping 145: No matching trig files found\n",
      "Skipping 146: No matching trig files found\n",
      "Skipping 147: No matching trig files found\n",
      "Skipping 148: No matching trig files found\n",
      "Skipping 149: No matching trig files found\n",
      "Skipping 150: No matching trig files found\n",
      "Skipping 151: No matching trig files found\n",
      "Skipping 152: No matching trig files found\n",
      "Skipping 153: No matching trig files found\n",
      "Skipping 154: No matching trig files found\n",
      "Skipping 155: No matching trig files found\n",
      "Skipping 158: No matching trig files found\n",
      "Skipping 159: No matching trig files found\n",
      "Skipping 160: No matching trig files found\n",
      "Skipping 161: No matching trig files found\n",
      "Skipping 162: No matching trig files found\n",
      "Skipping 163: No matching trig files found\n",
      "Skipping 164: No matching trig files found\n",
      "Skipping 165: No matching trig files found\n",
      "Skipping 166: No matching trig files found\n",
      "Skipping 167: No matching trig files found\n",
      "Skipping 168: No matching trig files found\n",
      "Skipping 169: No matching trig files found\n",
      "Skipping 170: No matching trig files found\n",
      "Skipping 171: No matching trig files found\n",
      "Skipping 172: No matching trig files found\n",
      "Skipping 173: No matching trig files found\n",
      "Skipping 174: No matching trig files found\n",
      "Skipping 175: No matching trig files found\n",
      "Skipping 176: No matching trig files found\n",
      "Skipping 177: No matching trig files found\n",
      "Skipping 179: No matching trig files found\n",
      "Skipping 180: No matching trig files found\n",
      "Skipping 181: No matching trig files found\n",
      "Skipping 182: No matching trig files found\n",
      "Skipping 183: No matching trig files found\n",
      "Skipping 184: No matching trig files found\n",
      "Skipping 185: No matching trig files found\n",
      "Skipping 186: No matching trig files found\n",
      "Skipping 187: No matching trig files found\n",
      "Skipping 188: No matching trig files found\n",
      "Skipping 189: No matching trig files found\n",
      "Skipping 190: No matching trig files found\n",
      "Skipping 191: No matching trig files found\n",
      "Skipping 192: No matching trig files found\n",
      "Skipping 193: No matching trig files found\n",
      "Skipping 195: No matching trig files found\n",
      "Skipping 197: No matching trig files found\n",
      "Skipping 198: No matching trig files found\n",
      "Skipping 199: No matching trig files found\n",
      "Skipping 200: No matching trig files found\n",
      "Skipping 201: No matching trig files found\n",
      "Skipping 203: No matching trig files found\n",
      "Skipping 204: No matching trig files found\n",
      "Skipping 205: No matching trig files found\n",
      "Skipping 206: No matching trig files found\n",
      "Skipping 207: No matching trig files found\n",
      "Skipping 208: No matching trig files found\n",
      "Skipping 209: No matching trig files found\n",
      "Skipping 210: No matching trig files found\n",
      "Skipping 211: No matching trig files found\n",
      "Skipping 212: No matching trig files found\n",
      "Skipping 213: No matching trig files found\n",
      "Skipping 214: No matching trig files found\n",
      "Skipping 215: No matching trig files found\n",
      "Skipping 216: No matching trig files found\n",
      "Skipping 217: No matching trig files found\n",
      "Skipping 219: No matching trig files found\n",
      "Skipping 220: No matching trig files found\n",
      "Skipping 221: No matching trig files found\n",
      "Skipping 222: No matching trig files found\n",
      "Skipping 223: No matching trig files found\n",
      "Skipping 224: No matching trig files found\n",
      "Skipping 225: No matching trig files found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import anthropic\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"\n",
    "    Function to generate key for natural sorting.\n",
    "    Splits string into list of strings and numbers.\n",
    "    \"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split('([0-9]+)', str(s))]\n",
    "\n",
    "class EventProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"Initialize EventProcessor with API key and create output directory.\"\"\"\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.output_dir = Path(\"rdf_output\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def collect_files(self, text_id: str, trig_dir: Path) -> Tuple[str, List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Collect text and trig files for a given text ID using natural sorting.\n",
    "        \n",
    "        Args:\n",
    "            text_id: The ID of the text file\n",
    "            trig_dir: Directory containing trig files\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing text content and lists of date and place file dictionaries\n",
    "        \"\"\"\n",
    "        text_file = Path(f\"./1891/1891/txt/{text_id}.txt\")\n",
    "        if not text_file.exists():\n",
    "            raise FileNotFoundError(f\"Text file {text_file} not found\")\n",
    "\n",
    "        with text_file.open('r', encoding='utf-8') as f:\n",
    "            text_content = f.read()\n",
    "\n",
    "        date_files = []\n",
    "        place_files = []\n",
    "\n",
    "        # Use natural sorting for trig files\n",
    "        trig_files = sorted(trig_dir.glob(f\"{text_id}_*.trig\"), \n",
    "                          key=lambda x: natural_sort_key(x.stem))\n",
    "        \n",
    "        for trig_file in trig_files:\n",
    "            with trig_file.open('r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                if 'date' in trig_file.stem.lower():\n",
    "                    date_files.append({'filename': trig_file.name, 'content': content})\n",
    "                elif 'place' in trig_file.stem.lower():\n",
    "                    place_files.append({'filename': trig_file.name, 'content': content})\n",
    "\n",
    "        return text_content, date_files, place_files\n",
    "\n",
    "    def extract_events(self, text: str, date_files: List[Dict], place_files: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract events using Claude API with both text and trig files.\n",
    "        \n",
    "        Args:\n",
    "            text: The diary text content\n",
    "            date_files: List of date trig file dictionaries\n",
    "            place_files: List of place trig file dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing extracted events\n",
    "        \"\"\"\n",
    "        system_prompt = \"\"\"Analyze the diary excerpt and the provided trig files to create a sequence of events representing Mary Berenson's movements in time and space. Each event should be created by matching places and dates from the text that are also represented in the trig files.\n",
    "\n",
    "For each event pair, you need to:\n",
    "1. Match places and dates that clearly belong together in the text\n",
    "2. Find the corresponding trig files for both the place and date\n",
    "3. Extract the UUIDs from the body annotations in the trig files (found in the pattern: annotation/UUID/body)\n",
    "4. Create a chronologically ordered sequence of events\n",
    "\n",
    "Return the results in this exact format, with no additional text:\n",
    "{\n",
    "    \"events\": [\n",
    "        {\n",
    "            \"event_value\": \"Place, day month year\",\n",
    "            \"place_annotation_uuid\": \"uuid-from-place-trig\",\n",
    "            \"date_annotation_uuid\": \"uuid-from-date-trig\",\n",
    "            \"event_uuid\": \"new-generated-uuid\",\n",
    "            \"next_event_value\": \"Place, day month year\",\n",
    "            \"next_event_uuid\": \"uuid-for-next-event\",\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "        content = f\"\"\"Here is the diary excerpt:\n",
    "        {text}\n",
    "\n",
    "        Here are the date trig files:\n",
    "        {'-' * 50}\n",
    "        \"\"\"\n",
    "\n",
    "        for date_file in sorted(date_files, key=lambda x: natural_sort_key(x['filename'])):\n",
    "            content += f\"\\nFile: {date_file['filename']}\\n{date_file['content']}\\n{'-' * 50}\"\n",
    "\n",
    "        content += \"\\n\\nHere are the place trig files:\\n\" + ('-' * 50)\n",
    "\n",
    "        for place_file in sorted(place_files, key=lambda x: natural_sort_key(x['filename'])):\n",
    "            content += f\"\\nFile: {place_file['filename']}\\n{place_file['content']}\\n{'-' * 50}\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=4096,\n",
    "                system=system_prompt,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Please analyze the following text and trig files to extract and match events:\\n\\n{content}\"\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            response_text = response.content[0].text\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "\n",
    "            if json_start == -1 or json_end == 0:\n",
    "                raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "            events_data = json.loads(response_text[json_start:json_end])\n",
    "            self.validate_event_uuid_sequence(events_data)\n",
    "            return events_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting events: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def validate_event_uuid_sequence(self, events_data: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Validate that next_event_uuid matches the event_uuid of the next event.\n",
    "        \n",
    "        Args:\n",
    "            events_data: Dictionary containing event data\n",
    "        \"\"\"\n",
    "        events = events_data.get('events', [])\n",
    "        for i in range(len(events) - 1):\n",
    "            current_event = events[i]\n",
    "            next_event = events[i + 1]\n",
    "\n",
    "            if current_event.get('next_event_uuid') != next_event.get('event_uuid'):\n",
    "                raise ValueError(\n",
    "                    f\"UUID sequence mismatch: Event {i}'s next_event_uuid \"\n",
    "                    f\"({current_event.get('next_event_uuid')}) does not match \"\n",
    "                    f\"Event {i+1}'s event_uuid ({next_event.get('event_uuid')})\"\n",
    "                )\n",
    "\n",
    "    def validate_uuids(self, events_data: Dict, date_files: List[Dict], place_files: List[Dict]) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that UUIDs in events match those in trig files.\n",
    "        \n",
    "        Args:\n",
    "            events_data: Dictionary containing event data\n",
    "            date_files: List of date trig file dictionaries\n",
    "            place_files: List of place trig file dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if all UUIDs are valid\n",
    "        \"\"\"\n",
    "        for event in events_data.get('events', []):\n",
    "            place_uuid = event.get('place_annotation_uuid')\n",
    "            date_uuid = event.get('date_annotation_uuid')\n",
    "\n",
    "            place_found = any(place_uuid in place_file['content'] for place_file in place_files)\n",
    "            date_found = any(date_uuid in date_file['content'] for date_file in date_files)\n",
    "\n",
    "            if not (place_found and date_found):\n",
    "                print(f\"UUID validation failed for event: {event['event_value']}\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def convert_to_trig(self, event_data: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Convert a single event data to Trig format.\n",
    "        \n",
    "        Args:\n",
    "            event_data: Dictionary containing event data\n",
    "            \n",
    "        Returns:\n",
    "            String containing Trig format data\n",
    "        \"\"\"\n",
    "        trig_template = f\"\"\"\n",
    "        @prefix crmdig: <http://www.ics.forth.gr/isl/CRMdig/> .\n",
    "@prefix crminfluence: <http://www.cidoc-crm.org/cidoc-crm/influence/> .\n",
    "@prefix oa: <http://www.w3.org/ns/oa#> .\n",
    "@prefix crmsci: <http://www.ics.forth.gr/isl/CRMsci/> .\n",
    "@prefix Help: <http://help.researchspace.org/resource/> .\n",
    "@prefix bds: <http://www.bigdata.com/rdf/search#> .\n",
    "@prefix crmba: <http://www.cidoc-crm.org/cidoc-crm/CRMba/> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix mbdiaries-annotation: <https://mbdiaries.itatti.harvard.edu/annotation/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix iiif: <http://iiif.io/api/> .\n",
    "@prefix crm: <http://www.cidoc-crm.org/cidoc-crm/> .\n",
    "@prefix sim: <http://purl.org/ontology/similarity/> .\n",
    "@prefix fc: <https://collection.itatti.harvard.edu/resource/custom/fc/> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix User: <http://www.researchspace.org/resource/user/> .\n",
    "@prefix mbdiaries-type: <https://mbdiaries.itatti.harvard.edu/resource/type/> .\n",
    "@prefix forms: <http://www.researchspace.org/resource/system/forms/> .\n",
    "@prefix owl: <http://www.w3.org/2002/07/owl#> .\n",
    "@prefix rshelp: <http://researchspace.org/help/> .\n",
    "@prefix sp: <http://spinrdf.org/sp#> .\n",
    "@prefix Platform: <http://www.researchspace.org/resource/system/> .\n",
    "@prefix mbdiaries: <https://mbdiaries.itatti.harvard.edu/resource/> .\n",
    "@prefix fr: <https://collection.itatti.harvard.edu/resource/custom/fr/> .\n",
    "@prefix crmgeo: <http://www.ics.forth.gr/isl/CRMgeo/> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix mbdiaries_forms: <http://mbdiaries.itatti.harvard.edu/resource/forms> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix rso: <http://www.researchspace.org/ontology/> .\n",
    "@prefix Admin: <http://www.researchspace.org/resource/admin/> .\n",
    "@prefix vitiiif: <https://iiif.itatti.harvard.edu/iiif/2/> .\n",
    "@prefix ontodia: <http://ontodia.org/schema/v1#> .\n",
    "@prefix frbroo: <http://iflastandards.info/ns/fr/frbr/frbroo/> .\n",
    "@prefix crmarchaeo: <http://www.cidoc-crm.org/cidoc-crm/CRMarchaeo/> .\n",
    "@prefix rsp: <http://www.researchspace.org/resource/> .\n",
    "@prefix Default: <https://collection.itatti.harvard.edu/resource/> .\n",
    "@prefix mbdiaries-document: <https://mbdiaries.itatti.harvard.edu/document/> .\n",
    "@prefix mbdiaries-ontology: <https://mbdiaries.itatti.harvard.edu/ontology/> .\n",
    "@prefix ldp: <http://www.w3.org/ns/ldp#> .\n",
    "\n",
    "        <https://mbdiaries.itatti.harvard.edu/event/{event_data.get('event_uuid', '')}/container/context> {{\n",
    "        <https://mbdiaries.itatti.harvard.edu/event/{event_data.get('event_uuid', '')}>\n",
    "            a crm:E5_event;\n",
    "            crm:P160_has_temporal_projection <https://mbdiaries.itatti.harvard.edu/annotation/{event_data.get('date_annotation_uuid', '')}/body>;\n",
    "            crm:P161_has_spatial_projection <https://mbdiaries.itatti.harvard.edu/annotation/{event_data.get('place_annotation_uuid', '')}/body>;\n",
    "            crm:P183_ends_before_the_start <https://mbdiaries.itatti.harvard.edu/event/{event_data.get('next_event_uuid', '')}>.\n",
    "            \n",
    "        _:node1i8224na8x5257 ldp:contains <https://mbdiaries.itatti.harvard.edu/event/{event_data.get('event_uuid', '')}/container> .\n",
    "              }}\n",
    "        {{\n",
    "        _:node1i8224na8x5257 a ldp:Container, ldp:Resource, prov:Entity .\n",
    "        }}\"\"\"\n",
    "        return trig_template\n",
    "\n",
    "    def save_events_as_trig(self, events_data: Dict, text_id: str):\n",
    "        \"\"\"\n",
    "        Save events to individual Trig files.\n",
    "        \n",
    "        Args:\n",
    "            events_id: The ID of the text file\n",
    "        \"\"\"\n",
    "        if not events_data or 'events' not in events_data:\n",
    "            print(\"No events to save\")\n",
    "            return\n",
    "\n",
    "        for i, event in enumerate(events_data['events'], 1):\n",
    "            trig_content = self.convert_to_trig(event)\n",
    "            trig_filename = self.output_dir / f\"{text_id}_event_{event['event_uuid']}.trig\"\n",
    "\n",
    "            try:\n",
    "                with trig_filename.open('w', encoding='utf-8') as f:\n",
    "                    f.write(trig_content)\n",
    "                print(f\"Saved Trig file: {trig_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving Trig file for event {i}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    api_key = \"\"\n",
    "    input_dir = Path(\"./txt\")\n",
    "    trig_dir = Path(\"rdf_output\")\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        processor = EventProcessor(api_key)\n",
    "\n",
    "        # Get naturally sorted list of all text files\n",
    "        text_files = sorted(list(input_dir.glob('*.txt')), \n",
    "                          key=lambda x: natural_sort_key(x.stem))\n",
    "        \n",
    "        # Process each text file\n",
    "        for i, text_file in enumerate(text_files):\n",
    "            text_id = text_file.stem\n",
    "            \n",
    "            # Collect date and place trig files for current text\n",
    "            trig_files = sorted(trig_dir.glob(f\"{text_id}_*.trig\"), \n",
    "                              key=lambda x: natural_sort_key(x.stem))\n",
    "            \n",
    "            date_files, place_files = [], []\n",
    "            for trig_file in trig_files:\n",
    "                with trig_file.open('r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    if 'date' in trig_file.stem.lower():\n",
    "                        date_files.append({'filename': trig_file.name, 'content': content})\n",
    "                    elif 'place' in trig_file.stem.lower():\n",
    "                        place_files.append({'filename': trig_file.name, 'content': content})\n",
    "\n",
    "            if not date_files or not place_files:\n",
    "                print(f\"Skipping {text_id}: No matching trig files found\")\n",
    "                continue\n",
    "\n",
    "            # Process current text\n",
    "            text_content = text_file.read_text(encoding='utf-8')\n",
    "            current_events_data = processor.extract_events(text_content, date_files, place_files)\n",
    "            \n",
    "            if not current_events_data or 'events' not in current_events_data: #first event in the next 3 texts\n",
    "                next_event = None\n",
    "            for j in range(i + 1, min(i + 4, len(text_files))):\n",
    "                next_text_file = text_files[j]\n",
    "                next_text_id = next_text_file.stem\n",
    "                \n",
    "                # Collect trig files for next text\n",
    "                next_trig_files = sorted(trig_dir.glob(f\"{next_text_id}_*.trig\"), \n",
    "                                       key=lambda x: natural_sort_key(x.stem))\n",
    "                \n",
    "                next_date_files, next_place_files = [], []\n",
    "                for trig_file in next_trig_files:\n",
    "                    with trig_file.open('r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        if 'date' in trig_file.stem.lower():\n",
    "                            next_date_files.append({'filename': trig_file.name, 'content': content})\n",
    "                        elif 'place' in trig_file.stem.lower():\n",
    "                            next_place_files.append({'filename': trig_file.name, 'content': content})\n",
    "\n",
    "                if next_date_files and next_place_files:\n",
    "                    next_text_content = next_text_file.read_text(encoding='utf-8')\n",
    "                    next_events_data = processor.extract_events(next_text_content, \n",
    "                                                             next_date_files, \n",
    "                                                             next_place_files)\n",
    "                    \n",
    "                    if next_events_data and 'events' in next_events_data and next_events_data['events']:\n",
    "                        next_event = next_events_data['events'][0]\n",
    "                        break\n",
    "\n",
    "            # Update the last event in current text with next event information\n",
    "            if next_event and current_events_data['events']:\n",
    "                last_event = current_events_data['events'][-1]\n",
    "                last_event['next_event_value'] = next_event['event_value']\n",
    "                last_event['next_event_uuid'] = next_event['event_uuid']\n",
    "\n",
    "            # Save processed events\n",
    "            output_json_path = output_dir / f\"{text_id}_events.json\"\n",
    "            with output_json_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(current_events_data, f, indent=2)\n",
    "\n",
    "            # Save events as Trig files\n",
    "            processor.save_events_as_trig(current_events_data, text_id)\n",
    "\n",
    "            print(f\"\\nProcessed {text_id}:\")\n",
    "            for event in current_events_data['events']:\n",
    "                print(f\"\\nEvent: {event['event_value']}\")\n",
    "                print(f\"Place UUID: {event['place_annotation_uuid']}\")\n",
    "                print(f\"Date UUID: {event['date_annotation_uuid']}\")\n",
    "                print(f\"Event UUID: {event['event_uuid']}\")\n",
    "                if 'next_event_value' in event:\n",
    "                    print(f\"Next Event: {event['next_event_value']}\")\n",
    "                    print(f\"Next Event UUID: {event['next_event_uuid']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        print(\"Please ensure:\")\n",
    "        print(\"1. The 'input' directory exists\")\n",
    "        print(\"2. Text and trig files are in the input directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
